{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anti-Money Laundering and Fraud Prediction\n",
    "This is a breakdown of the overall models that can be developed to make AI models to predict and detect money-laundering or fraud within financial datasets.\n",
    "Data sources for these datasets come from sources on Kaggle:\n",
    "- [Credit Card Fraud Detection | Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?datasetId=310&sortBy=voteCount)\n",
    "- [Fake Bills | Kaggle](\"https://www.kaggle.com/datasets/alexandrepetit881234/fake-bills\")\n",
    "\n",
    "These data sets contain various amounts of data dating till 2013, with various levels of information that is captured from financial entities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages to install for below\n",
    "> Make sure pip is up to date for these packages to install\n",
    ">> `python.exe -m pip install --upgrade pip`\n",
    "\n",
    "> To install the SciKit (sklearn) packages use the below command:\n",
    ">> `pip install scikit-learn`\n",
    "\n",
    "> To install Seaborn packages use the below command:\n",
    ">> `pip install seaborn`\n",
    "\n",
    "> To install the Plotly packages use the below command:\n",
    ">> `pip install plotly`\n",
    "\n",
    "NOTE: You will need to download Python version 3.11 from the Microsoft Store for this to work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "To start the overall work click play on the play button for the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra breakdown\n",
    "import pandas as pd # data processing, CSV files input/output\n",
    "import matplotlib.pyplot as plt # graph plotting\n",
    "import seaborn as sns \n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from numpy import percentile\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "from scipy.stats import trim_mean\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above components you will then be able to import the different files that are needing to be analysed with Pandas.\n",
    "Pandas will be able to pull in the different files, for example with this work from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block would allow for pulling in multiple different sources, but these are not all the same formats, so will throw errors\n",
    "#urls = [\"https://github.com/jono120/fictional-octo-potato/raw/main/transaction_data/fake_bills.csv\", \"https://github.com/Jono120/fictional-octo-potato/tree/main/transaction_data/bank.csv\", \"https://github.com/Jono120/fictional-octo-potato/tree/main/transaction_data/train_trd.csv\"]\n",
    "#df = pd.read_csv(\"https://github.com/jono120/fictional-octo-potato/raw/main/transaction_data/fake_bills.csv\")\n",
    "#df = pd.read_csv(\"https://github.com/Jono120/fictional-octo-potato/tree/main/transaction_data/bank.csv\")\n",
    "#df = pd.read_csv(\"https://github.com/Jono120/fictional-octo-potato/tree/main/transaction_data/train_trd.csv\")\n",
    "\n",
    "#dfs = [pd.read_csv(url) for url in urls]\n",
    "#df = pd.concat(dfs)\n",
    "#print(df.head())\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://github.com/jono120/fictional-octo-potato/raw/main/transaction_data/fake_bills.csv\", sep = ';')\n",
    "\n",
    "df_false = df.loc[df['is_genuine']==False]\n",
    "df_true = df.loc[df['is_genuine']==True]\n",
    "df_true = df_true.fillna(df_true.median())\n",
    "df_false = df_false.fillna(df_false.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"Overview of the data:\" )\n",
    "print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_false.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catfeat = []\n",
    "numfeat = []\n",
    "\n",
    "for i in df.columns:\n",
    "    if(df[i].dtypes == 'objects'): catfeat.append(i)\n",
    "    else:\n",
    "        numfeat.append(i)\n",
    "print(f'The number of Objects Features : {len(catfeat)}')\n",
    "print(f'The number of Numerical Features : {len(numfeat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of missing values : {df.isnull().sum().sum()}')\n",
    "\n",
    "# This might not work, but lets try it\n",
    "object_cols = [\"\"]\n",
    "for i in object_cols:\n",
    "    print(\"column name : {}\".format(i))\n",
    "    print(\"NUmber of unique columns of \", i, \":{}\".format(df[i].nunique()))\n",
    "    print(\"Values of unique columns of \", i, \"is below: \\n{}\".format(df[i].value_counts()))\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedfeat = ['Time', 'Amount']\n",
    "for i in df[namedfeat]:\n",
    "    if(df[i].duplicated().sum() > 0): print(f'{i} Number of Duplicatied : {df[i].duplicated().sum()}') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot Graphs\n",
    "This will showcase the data as scatter plot graphs, in both 3D and 2D styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(df, x='margin_low', y='length', z='height_left', color='is_genuine')\n",
    "px.scatter_3d(df, x='length', y='margin_low', color='is_genuine')\n",
    "px.scatter_3d(df, x='length', y='margin_up', color='is_genuine')\n",
    "px.scatter_3d(df, x='length', y='height_left', color='is_genuine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x='margin_low', y='length', z='height_left', color='is_genuine')\n",
    "px.scatter(df, x='length', y='margin_low', color='is_genuine')\n",
    "px.scatter(df, x='length', y='margin_up', color='is_genuine')\n",
    "px.scatter(df, x='length', y='height_left', color='is_genuine')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrich Data\n",
    "This will use the TensorFlow platform to enrich the data and analyse information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(40, input_dim=6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[0:, 1:].values\n",
    "y1 = df['is_genuine']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c411529d52bd385a0c5cc81dc7774bf283ab520578901012898b94c5b56fc2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

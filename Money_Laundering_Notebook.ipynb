{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anti-Money Laundering and Fraud Prediction\n",
    "This is a breakdown of the overall models that can be developed to make AI models to predict and detect money-laundering or fraud within financial datasets.\n",
    "Data sources for these datasets come from sources on Kaggle:\n",
    "- [Credit Card Fraud Detection | Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?datasetId=310&sortBy=voteCount)\n",
    "- [Fake Bills | Kaggle](\"https://www.kaggle.com/datasets/alexandrepetit881234/fake-bills\")\n",
    "\n",
    "These data sets contain various amounts of data dating till 2013, with various levels of information that is captured from financial entities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages to install for below\n",
    "> Make sure pip is up to date for these packages to install\n",
    ">> `python.exe -m pip install --upgrade pip`\n",
    "\n",
    "> To install the SciKit (sklearn) packages use the below command:\n",
    ">> `pip install scikit-learn`\n",
    "\n",
    "> To install Seaborn packages use the below command:\n",
    ">> `pip install seaborn`\n",
    "\n",
    "> To install the Plotly packages use the below command:\n",
    ">> `pip install plotly`\n",
    "\n",
    "> To install the Tensorflow packages use the below command:\n",
    ">> `pip install tensorflow`\n",
    "\n",
    "NOTE: You will need to download Python version 3.11 from the Microsoft Store for this to work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "To start the overall work click play on the play button for the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np # linear algebra breakdown\n",
    "import pandas as pd # data processing, CSV files input/output\n",
    "import matplotlib.pyplot as plt # graph plotting\n",
    "import seaborn as sns \n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "from numpy import percentile\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "from scipy.stats import trim_mean\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above components you will then be able to import the different files that are needing to be analysed with Pandas.\n",
    "Pandas will be able to pull in the different files, for example with this work from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block would allow for pulling in multiple different sources, but these are not all the same formats, so will throw errors\n",
    "#urls = [\"https://github.com/jono120/fictional-octo-potato/raw/main/transaction_data/fake_bills.csv\", \"https://github.com/Jono120/fictional-octo-potato/tree/main/transaction_data/bank.csv\", \"https://github.com/Jono120/fictional-octo-potato/tree/main/transaction_data/train_trd.csv\"]\n",
    "#df = pd.read_csv(\"https://github.com/jono120/fictional-octo-potato/raw/main/transaction_data/fake_bills.csv\")\n",
    "#df = pd.read_csv(\"https://github.com/Jono120/fictional-octo-potato/tree/main/transaction_data/bank.csv\")\n",
    "#df = pd.read_csv(\"https://github.com/Jono120/fictional-octo-potato/tree/main/transaction_data/train_trd.csv\")\n",
    "\n",
    "#dfs = [pd.read_csv(url) for url in urls]\n",
    "#df = pd.concat(dfs)\n",
    "#print(df.head())\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"https://github.com/jono120/fictional-octo-potato/raw/main/transaction_data/fake_bills.csv\", sep = ';')\n",
    "df = pd.read_csv(\"https://github.com/Jono120/fictional-octo-potato/raw/main/transaction_data/bank.csv\")\n",
    "\n",
    "# Checking the file structure\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of dataset\n",
    "This shows the breakdown of the dataset and shows what is visible in the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"Overview of the data:\")\n",
    "print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data types of columns\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Description of the dataset:\")\n",
    "df.info().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scans the data to search the percentages of fraud vs no fraud\n",
    "amount = df.groupby('BALANCE AMT')['BALANCE AMT'].sum()\n",
    "fraud, unfraud = len(df[df['BALANCE AMT'] == 1]), len(df[df['BALANCE AMT'] == 0])\n",
    "fraud_perc, unfraud_perc = (fraud/len(df)) * 100, (unfraud/len(df))*100\n",
    "\n",
    "Loss = pd.DataFrame({'Fraud' : ['Fraud', 'No Fraud'], 'Total Amount' : [amount[1], amount[0]], 'Freq.' : [fraud, unfraud], '% perc.' : [fraud_perc, unfraud_perc]})\n",
    "\n",
    "Loss = Loss.set_index('Fraud')\n",
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalise the lists to store catagorical and numerical features\n",
    "catfeat = []\n",
    "numfeat = []\n",
    "\n",
    "for i in df.columns:\n",
    "    if(df[i].dtypes == 'BALANCE AMT'): catfeat.append(i)\n",
    "    else:\n",
    "        numfeat.append(i)\n",
    "print(f'The number of Objects Features : {len(catfeat)}')\n",
    "print(f'The number of Numerical Features : {len(numfeat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of missing values : {df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This scans the data for any duplicates that are within the datasets\n",
    "namedfeat = [' WITHDRAWAL AMT ', ' DEPOSIT AMT ', 'BALANCE AMT']\n",
    "for i in df[namedfeat]:\n",
    "    if(df[i].duplicated().sum() > 0): print(f'{i} has {df[i].duplicated().sum()} duplicates') \n",
    "\n",
    "df[namedfeat].describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the data variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage Scores\n",
    "This should give a breakdown of the successful percentage amounts for each of the two columns referenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the quality of the withdrawal amounts in the dataset\n",
    "class_counts_with = df[' WITHDRAWAL AMT '].value_counts()\n",
    "class_counts_percentage_with = df[' WITHDRAWAL AMT '].value_counts(normalize=True) * 100\n",
    "\n",
    "# Checks the quality of the deposit amounts in the dataset\n",
    "class_counts_dep = df[' DEPOSIT AMT '].value_counts()\n",
    "class_counts_percentage_dep = df[' DEPOSIT AMT '].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Withdrawal amounts: \\n\", class_counts_with)\n",
    "print(\"Deposit amounts: \\n\", class_counts_dep)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Plot graph\n",
    "This allows for the creation of a bar graph using specific categorical information for the overall datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=' WITHDRAWAL AMT ', data=df)\n",
    "\n",
    "# Add title, x-axis, y-axis labels for the graph\n",
    "plt.title(\"Distributions of the targets\")\n",
    "plt.xlabel(\" WITHDRAWAL AMT \")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "class_counts = df[' WITHDRAWAL AMT '].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot Graphs\n",
    "This will showcase the data as scatter plot graphs, in both 3D and 2D styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "px.scatter(df, x =' WITHDRAWAL AMT ', y =' DEPOSIT AMT ', color ='TRANSACTION DETAILS')\n",
    "#px.scatter(df, x='length', y='margin_low', color='is_genuine')\n",
    "#px.scatter(df, x='length', y='margin_up', color='is_genuine')\n",
    "#px.scatter(df, x='length', y='height_left', color='is_genuine')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Enrichments\n",
    "This will use the TensorFlow platform to enrich the data and analyse information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(40, input_dim=6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[0:, 1:].values\n",
    "y = df['TRANSACTION DETAILS']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "This will scan through the data, to process the data for allowing training and test data to be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X = df.drop(' WITHDRAWAL AMT ', axis=1)\n",
    "y = df[' DEPOSIT AMT ']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Splits\n",
    "This section will pull all information that is needed for building out the Training and Testing models within the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations\n",
    "The below section will enable the information to be transformed into the relevant testing and training information, for further graphing and data sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train[namedfeat] = StandardScaler().fit_transform(X_train[namedfeat])\n",
    "X_test[namedfeat] = StandardScaler().fit_transform(X_test[namedfeat])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c411529d52bd385a0c5cc81dc7774bf283ab520578901012898b94c5b56fc2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
